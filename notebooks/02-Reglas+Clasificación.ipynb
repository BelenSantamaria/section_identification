{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ff0af4-0da0-45a6-abd7-dc6396eff79a",
   "metadata": {},
   "source": [
    "# **02-Reglas+Clasificaci贸n**\n",
    "\n",
    "Referencias\n",
    "\n",
    "* [Pysbd](https://github.com/nipunsadvilkar/pySBD/tree/master)\n",
    "* [Hyperparameter tuning transformers](https://huggingface.co/docs/transformers/hpo_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4882fa06-e7de-401d-b193-827f483e3f73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/belensantamaria/Documentos/section_identification\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b47dada4-22a2-45fa-aaf9-d09de0b498cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "import pysbd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a05ec3e-3285-45e3-bc13-3e7a41431dcd",
   "metadata": {},
   "source": [
    "## Creaci贸n del conjunto de datos para entrenar el clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea43fc0-1690-4a30-857b-62daa20e4ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_df(file_path):\n",
    "\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    entries = []\n",
    "    for note_id, entry in data[\"annotated_entries\"].items():\n",
    "        for annotation in entry[\"section_annotation\"][\"gold\"]:\n",
    "            d = {\n",
    "                \"text\": annotation[\"segment\"],\n",
    "                \"label\": annotation[\"label\"],\n",
    "            }\n",
    "            entries.append(d)\n",
    "        \n",
    "    df = pd.DataFrame(entries)    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcac0add-a5c5-403b-9de1-2e461ca4113f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path = \"data/raw/clinais.train.json\"\n",
    "df_train = json_to_df(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57e7a743-1ad3-42ac-bf6b-bb6496d5ec47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>En Mayo de 1997, una mujer de 29 a帽os de edad ...</td>\n",
       "      <td>PRESENT_ILLNESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>la ecograf铆a y la tomograf铆a axial computeriza...</td>\n",
       "      <td>EXPLORATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Se realiz贸 resecci贸n completa de la tumoraci贸n...</td>\n",
       "      <td>TREATMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Treinta meses despu茅s, la paciente present贸 en...</td>\n",
       "      <td>EVOLUTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>se reintervino y se llev贸 a cabo una resecci贸n...</td>\n",
       "      <td>TREATMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6471</th>\n",
       "      <td>por lo que precis贸 anticoagulaci贸n y retirada ...</td>\n",
       "      <td>TREATMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6472</th>\n",
       "      <td>A pesar de la utilizaci贸n de la fistula arteri...</td>\n",
       "      <td>EVOLUTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6473</th>\n",
       "      <td>y por petici贸n de la paciente, se replante贸 vo...</td>\n",
       "      <td>TREATMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6474</th>\n",
       "      <td>Diariamente tiene una ultrafiltrafiltraci贸n co...</td>\n",
       "      <td>EVOLUTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6475</th>\n",
       "      <td>Las gu铆as SEN recomiendan realizar reentrenami...</td>\n",
       "      <td>TREATMENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6476 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text            label\n",
       "0     En Mayo de 1997, una mujer de 29 a帽os de edad ...  PRESENT_ILLNESS\n",
       "1     la ecograf铆a y la tomograf铆a axial computeriza...      EXPLORATION\n",
       "2     Se realiz贸 resecci贸n completa de la tumoraci贸n...        TREATMENT\n",
       "3     Treinta meses despu茅s, la paciente present贸 en...        EVOLUTION\n",
       "4     se reintervino y se llev贸 a cabo una resecci贸n...        TREATMENT\n",
       "...                                                 ...              ...\n",
       "6471  por lo que precis贸 anticoagulaci贸n y retirada ...        TREATMENT\n",
       "6472  A pesar de la utilizaci贸n de la fistula arteri...        EVOLUTION\n",
       "6473  y por petici贸n de la paciente, se replante贸 vo...        TREATMENT\n",
       "6474  Diariamente tiene una ultrafiltrafiltraci贸n co...        EVOLUTION\n",
       "6475  Las gu铆as SEN recomiendan realizar reentrenami...        TREATMENT\n",
       "\n",
       "[6476 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.to_json(\"data/rule/train.json\")\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed19adaa-7ce3-4f49-a55b-12d04396113a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5400e90-8bbe-40da-824c-ded4b1807df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"label\"] = le.transform(df_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d921d1f-774c-4288-a4ea-dca7dbe8051c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>En Mayo de 1997, una mujer de 29 a帽os de edad ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>la ecograf铆a y la tomograf铆a axial computeriza...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Se realiz贸 resecci贸n completa de la tumoraci贸n...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Treinta meses despu茅s, la paciente present贸 en...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>se reintervino y se llev贸 a cabo una resecci贸n...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6471</th>\n",
       "      <td>por lo que precis贸 anticoagulaci贸n y retirada ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6472</th>\n",
       "      <td>A pesar de la utilizaci贸n de la fistula arteri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6473</th>\n",
       "      <td>y por petici贸n de la paciente, se replante贸 vo...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6474</th>\n",
       "      <td>Diariamente tiene una ultrafiltrafiltraci贸n co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6475</th>\n",
       "      <td>Las gu铆as SEN recomiendan realizar reentrenami...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6476 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     En Mayo de 1997, una mujer de 29 a帽os de edad ...      5\n",
       "1     la ecograf铆a y la tomograf铆a axial computeriza...      2\n",
       "2     Se realiz贸 resecci贸n completa de la tumoraci贸n...      6\n",
       "3     Treinta meses despu茅s, la paciente present贸 en...      1\n",
       "4     se reintervino y se llev贸 a cabo una resecci贸n...      6\n",
       "...                                                 ...    ...\n",
       "6471  por lo que precis贸 anticoagulaci贸n y retirada ...      6\n",
       "6472  A pesar de la utilizaci贸n de la fistula arteri...      1\n",
       "6473  y por petici贸n de la paciente, se replante贸 vo...      6\n",
       "6474  Diariamente tiene una ultrafiltrafiltraci贸n co...      1\n",
       "6475  Las gu铆as SEN recomiendan realizar reentrenami...      6\n",
       "\n",
       "[6476 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d8c9bd-57d0-4c2a-9ba7-134149d70ecb",
   "metadata": {},
   "source": [
    "## Creo secciones mediante reglas para el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a333e4b-2075-4e62-8a81-dcd2df723b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"data/raw/clinais.dev.json\"\n",
    "\n",
    "with open(test_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "sections = []\n",
    "seg = pysbd.Segmenter(language=\"es\", clean=False, char_span=True)\n",
    "\n",
    "for note_id, entry in data[\"annotated_entries\"].items():\n",
    "    for sec in seg.segment(entry[\"note_text\"]):\n",
    "        d = {\n",
    "            \"note_id\": note_id,\n",
    "            \"text\": sec.sent,\n",
    "            # Necesito columna de label pero le pongo 0 a todo y luego lo cambio con las predicciones\n",
    "            \"label\": 0,\n",
    "            \"start_offset\": sec.start,\n",
    "            \"end_offset\": sec.end,\n",
    "        }\n",
    "        sections.append(d)\n",
    "\n",
    "df_test = pd.DataFrame(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b68b89f6-c9f9-4e84-a83e-b4bea4f042cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>start_offset</th>\n",
       "      <th>end_offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S0004-06142005000200009-3</td>\n",
       "      <td>Paciente de 69 a. de edad con un PSA en el mom...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S0004-06142005000200009-3</td>\n",
       "      <td>El paciente ten铆a una biopsia previa por sexta...</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S0004-06142005000200009-3</td>\n",
       "      <td>Se practic贸 una E-RME que mostr贸 inicialmente ...</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S0004-06142005001000015-1</td>\n",
       "      <td>Un paciente var贸n de 19 a帽os acudi贸 al Servici...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S0004-06142005001000015-1</td>\n",
       "      <td>No presentaba ning煤n antecedente urol贸gico.</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>S1135-76062007000100006-1</td>\n",
       "      <td>La madre hab铆a fallecido s煤bitamente a los 48 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>S1135-76062007000100006-1</td>\n",
       "      <td>Una noche sali贸 a cenar con los compa帽eros de ...</td>\n",
       "      <td>0</td>\n",
       "      <td>256</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>S1135-76062007000100006-1</td>\n",
       "      <td>No manifest贸 ninguna sintomatolog铆a y se acost...</td>\n",
       "      <td>0</td>\n",
       "      <td>369</td>\n",
       "      <td>439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>S1135-76062007000100006-1</td>\n",
       "      <td>Poco despu茅s el perro comenz贸 a ladrar por lo ...</td>\n",
       "      <td>0</td>\n",
       "      <td>439</td>\n",
       "      <td>537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>S1135-76062007000100006-1</td>\n",
       "      <td>Se avis贸 a los servicios de emergencia 061 que...</td>\n",
       "      <td>0</td>\n",
       "      <td>537</td>\n",
       "      <td>629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1989 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        note_id  \\\n",
       "0     S0004-06142005000200009-3   \n",
       "1     S0004-06142005000200009-3   \n",
       "2     S0004-06142005000200009-3   \n",
       "3     S0004-06142005001000015-1   \n",
       "4     S0004-06142005001000015-1   \n",
       "...                         ...   \n",
       "1984  S1135-76062007000100006-1   \n",
       "1985  S1135-76062007000100006-1   \n",
       "1986  S1135-76062007000100006-1   \n",
       "1987  S1135-76062007000100006-1   \n",
       "1988  S1135-76062007000100006-1   \n",
       "\n",
       "                                                   text  label  start_offset  \\\n",
       "0     Paciente de 69 a. de edad con un PSA en el mom...      0             0   \n",
       "1     El paciente ten铆a una biopsia previa por sexta...      0            79   \n",
       "2     Se practic贸 una E-RME que mostr贸 inicialmente ...      0           140   \n",
       "3     Un paciente var贸n de 19 a帽os acudi贸 al Servici...      0             0   \n",
       "4          No presentaba ning煤n antecedente urol贸gico.       0           112   \n",
       "...                                                 ...    ...           ...   \n",
       "1984  La madre hab铆a fallecido s煤bitamente a los 48 ...      0           180   \n",
       "1985  Una noche sali贸 a cenar con los compa帽eros de ...      0           256   \n",
       "1986  No manifest贸 ninguna sintomatolog铆a y se acost...      0           369   \n",
       "1987  Poco despu茅s el perro comenz贸 a ladrar por lo ...      0           439   \n",
       "1988  Se avis贸 a los servicios de emergencia 061 que...      0           537   \n",
       "\n",
       "      end_offset  \n",
       "0             79  \n",
       "1            140  \n",
       "2            479  \n",
       "3            112  \n",
       "4            156  \n",
       "...          ...  \n",
       "1984         256  \n",
       "1985         369  \n",
       "1986         439  \n",
       "1987         537  \n",
       "1988         629  \n",
       "\n",
       "[1989 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c476d4-3b52-4995-bc8e-e99a6f3d0667",
   "metadata": {},
   "source": [
    "## B煤squeda hiperpar谩metros transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0de09ab0-279d-4d22-a6c3-87dd579dcb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"PlanTL-GOB-ES/roberta-base-biomedical-clinical-es\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdb1fe1f-23e8-4a8d-968c-00bef40a3007",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e17efd79-e3b2-4bd5-83d2-e864782e0036",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_train.label.unique().tolist()\n",
    "train, valid = train_test_split(df_train, test_size=0.2)\n",
    "\n",
    "dataset = DatasetDict()\n",
    "\n",
    "dataset[\"train\"] = Dataset.from_pandas(train)\n",
    "dataset[\"valid\"] = Dataset.from_pandas(valid)\n",
    "dataset[\"test\"] = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7624ba5f-b08b-4114-b5e8-1c2d8f3d918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da806b2a-1cb6-4384-bef5-ab199e49db0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5c2686985244b191440443307fe892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5180 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b9474dea824f799fe8621213169d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1296 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f8c07a6be24e0494fa7765c36d3ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1989 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_data = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "548694c8-38e6-4a15-92cd-5a91f69dc5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59a75d78-44b2-4df6-ab5b-8601e6871574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/belensantamaria/opt/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/training_args.py:1759: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of  Transformers.`mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at PlanTL-GOB-ES/roberta-base-biomedical-clinical-es and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(set(dataset[\"train\"][\"label\"]))\n",
    "\n",
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"data/rule\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    load_best_model_at_end=False,\n",
    "    save_total_limit=10,\n",
    "    use_mps_device=True,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return metric_acc.compute(predictions=preds, references=labels)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=encoded_data[\"train\"],\n",
    "    eval_dataset=encoded_data[\"valid\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "687a3ad7-f2df-4a9e-b6af-07c0c0f9acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_space_optuna(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-4),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 3),\n",
    "        \"seed\": trial.suggest_int(\"seed\", 1, 40),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0, 0.1),\n",
    "        \"lr_scheduler_type\": trial.suggest_categorical(\"lr_scheduler_type\", [\"cosine_with_restarts\", \"linear\"]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9002e5bc-e7ad-4f91-a877-6ec80470a800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-21 14:11:10,860] A new study created in memory with name: no-name-4ccffefd-d965-4104-ba31-f0d14cd2f45d\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at PlanTL-GOB-ES/roberta-base-biomedical-clinical-es and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/belensantamaria/opt/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1296' max='1296' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1296/1296 1:27:00, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.506600</td>\n",
       "      <td>0.325485</td>\n",
       "      <td>0.909722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.223000</td>\n",
       "      <td>0.306686</td>\n",
       "      <td>0.928241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-21 15:38:16,213] Trial 0 finished with value: 0.9282407407407407 and parameters: {'learning_rate': 4.9989358625559586e-05, 'num_train_epochs': 2, 'seed': 38, 'weight_decay': 0.08984958679102431, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 0 with value: 0.9282407407407407.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at PlanTL-GOB-ES/roberta-base-biomedical-clinical-es and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/belensantamaria/opt/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='648' max='648' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [648/648 47:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.478000</td>\n",
       "      <td>0.258592</td>\n",
       "      <td>0.924383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-21 16:25:39,496] Trial 1 finished with value: 0.9243827160493827 and parameters: {'learning_rate': 7.226361651030256e-05, 'num_train_epochs': 1, 'seed': 37, 'weight_decay': 0.05020773214857537, 'lr_scheduler_type': 'linear'}. Best is trial 0 with value: 0.9282407407407407.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at PlanTL-GOB-ES/roberta-base-biomedical-clinical-es and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/belensantamaria/opt/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1296' max='1296' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1296/1296 1:25:41, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.524300</td>\n",
       "      <td>0.264780</td>\n",
       "      <td>0.925926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.235000</td>\n",
       "      <td>0.306378</td>\n",
       "      <td>0.925926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-21 17:51:27,022] Trial 2 finished with value: 0.9259259259259259 and parameters: {'learning_rate': 2.6822551582114098e-05, 'num_train_epochs': 2, 'seed': 16, 'weight_decay': 0.043659791302062156, 'lr_scheduler_type': 'linear'}. Best is trial 0 with value: 0.9282407407407407.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at PlanTL-GOB-ES/roberta-base-biomedical-clinical-es and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/belensantamaria/opt/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1944' max='1944' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1944/1944 2:22:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.563300</td>\n",
       "      <td>0.384145</td>\n",
       "      <td>0.898920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.283900</td>\n",
       "      <td>0.343865</td>\n",
       "      <td>0.919753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.182300</td>\n",
       "      <td>0.370846</td>\n",
       "      <td>0.925926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-21 20:13:34,786] Trial 3 finished with value: 0.9259259259259259 and parameters: {'learning_rate': 8.6201919717379e-05, 'num_train_epochs': 3, 'seed': 6, 'weight_decay': 0.021857402537814354, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 0 with value: 0.9282407407407407.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at PlanTL-GOB-ES/roberta-base-biomedical-clinical-es and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/belensantamaria/opt/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='648' max='648' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [648/648 47:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.465500</td>\n",
       "      <td>0.281827</td>\n",
       "      <td>0.914352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-21 21:01:18,190] Trial 4 finished with value: 0.9143518518518519 and parameters: {'learning_rate': 3.856308137687861e-05, 'num_train_epochs': 1, 'seed': 29, 'weight_decay': 0.08584661871656707, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 0 with value: 0.9282407407407407.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at PlanTL-GOB-ES/roberta-base-biomedical-clinical-es and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/belensantamaria/opt/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='648' max='648' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [648/648 46:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.484300</td>\n",
       "      <td>0.269903</td>\n",
       "      <td>0.918981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-21 21:47:31,471] Trial 5 finished with value: 0.9189814814814815 and parameters: {'learning_rate': 7.354848730586093e-05, 'num_train_epochs': 1, 'seed': 12, 'weight_decay': 0.016830105494168756, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 0 with value: 0.9282407407407407.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at PlanTL-GOB-ES/roberta-base-biomedical-clinical-es and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/belensantamaria/opt/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='648' max='648' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [648/648 46:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.514800</td>\n",
       "      <td>0.257811</td>\n",
       "      <td>0.927469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-21 22:33:57,357] Trial 6 finished with value: 0.9274691358024691 and parameters: {'learning_rate': 3.035442787101334e-05, 'num_train_epochs': 1, 'seed': 31, 'weight_decay': 0.028974526212703657, 'lr_scheduler_type': 'linear'}. Best is trial 0 with value: 0.9282407407407407.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at PlanTL-GOB-ES/roberta-base-biomedical-clinical-es and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/belensantamaria/opt/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='648' max='1944' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 648/1944 45:42 < 1:31:41, 0.24 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.500800</td>\n",
       "      <td>0.353658</td>\n",
       "      <td>0.905093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-21 23:19:45,529] Trial 7 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at PlanTL-GOB-ES/roberta-base-biomedical-clinical-es and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/belensantamaria/opt/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='648' max='1944' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 648/1944 42:26 < 1:25:07, 0.25 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.523900</td>\n",
       "      <td>0.456264</td>\n",
       "      <td>0.884259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-22 00:02:17,161] Trial 8 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at PlanTL-GOB-ES/roberta-base-biomedical-clinical-es and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/belensantamaria/opt/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='648' max='648' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [648/648 41:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.526000</td>\n",
       "      <td>0.295898</td>\n",
       "      <td>0.906636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-22 00:43:59,639] Trial 9 pruned. \n"
     ]
    }
   ],
   "source": [
    "best_run = trainer.hyperparameter_search(hp_space=hp_space_optuna, n_trials=10, direction=\"maximize\", backend=\"optuna\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03b16a2d-abf9-4d58-85f6-df22058f1549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BestRun(run_id='0', objective=0.9282407407407407, hyperparameters={'learning_rate': 4.9989358625559586e-05, 'num_train_epochs': 2, 'seed': 38, 'weight_decay': 0.08984958679102431, 'lr_scheduler_type': 'cosine_with_restarts'}, run_summary=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c6d268-a89f-4550-bce9-56d569b0f7c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predicci贸n de las secciones del conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cabdf677-94ed-488f-bf5f-eeba4eb80b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"PlanTL-GOB-ES/roberta-base-biomedical-clinical-es\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "best_model = AutoModelForSequenceClassification.from_pretrained(\"data/rule/run-0/checkpoint-1000\")\n",
    "\n",
    "best_trainer = Trainer(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0beac165-75e7-48fc-879e-df93a49df7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, labels, _ = best_trainer.predict(encoded_data[\"test\"])\n",
    "original_labels = le.inverse_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d501f3e-c495-47c9-884d-8c0491b89b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DERIVED_FROM/TO', 'DERIVED_FROM/TO', 'DERIVED_FROM/TO', ...,\n",
       "       'DERIVED_FROM/TO', 'DERIVED_FROM/TO', 'DERIVED_FROM/TO'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad322454-a36e-4c81-8eac-db6332b05f01",
   "metadata": {},
   "source": [
    "## Transformaci贸n de los segmentos y etiquetas para su evaluaci贸n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d6ac1de-f2c0-4080-bc53-9d1d8f31125c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>start_offset</th>\n",
       "      <th>end_offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S0004-06142005000200009-3</td>\n",
       "      <td>Paciente de 69 a. de edad con un PSA en el mom...</td>\n",
       "      <td>DERIVED_FROM/TO</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S0004-06142005000200009-3</td>\n",
       "      <td>El paciente ten铆a una biopsia previa por sexta...</td>\n",
       "      <td>DERIVED_FROM/TO</td>\n",
       "      <td>79</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S0004-06142005000200009-3</td>\n",
       "      <td>Se practic贸 una E-RME que mostr贸 inicialmente ...</td>\n",
       "      <td>DERIVED_FROM/TO</td>\n",
       "      <td>140</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S0004-06142005001000015-1</td>\n",
       "      <td>Un paciente var贸n de 19 a帽os acudi贸 al Servici...</td>\n",
       "      <td>DERIVED_FROM/TO</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S0004-06142005001000015-1</td>\n",
       "      <td>No presentaba ning煤n antecedente urol贸gico.</td>\n",
       "      <td>DERIVED_FROM/TO</td>\n",
       "      <td>112</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>S1135-76062007000100006-1</td>\n",
       "      <td>La madre hab铆a fallecido s煤bitamente a los 48 ...</td>\n",
       "      <td>DERIVED_FROM/TO</td>\n",
       "      <td>180</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>S1135-76062007000100006-1</td>\n",
       "      <td>Una noche sali贸 a cenar con los compa帽eros de ...</td>\n",
       "      <td>DERIVED_FROM/TO</td>\n",
       "      <td>256</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>S1135-76062007000100006-1</td>\n",
       "      <td>No manifest贸 ninguna sintomatolog铆a y se acost...</td>\n",
       "      <td>DERIVED_FROM/TO</td>\n",
       "      <td>369</td>\n",
       "      <td>439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>S1135-76062007000100006-1</td>\n",
       "      <td>Poco despu茅s el perro comenz贸 a ladrar por lo ...</td>\n",
       "      <td>DERIVED_FROM/TO</td>\n",
       "      <td>439</td>\n",
       "      <td>537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>S1135-76062007000100006-1</td>\n",
       "      <td>Se avis贸 a los servicios de emergencia 061 que...</td>\n",
       "      <td>DERIVED_FROM/TO</td>\n",
       "      <td>537</td>\n",
       "      <td>629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1989 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        note_id  \\\n",
       "0     S0004-06142005000200009-3   \n",
       "1     S0004-06142005000200009-3   \n",
       "2     S0004-06142005000200009-3   \n",
       "3     S0004-06142005001000015-1   \n",
       "4     S0004-06142005001000015-1   \n",
       "...                         ...   \n",
       "1984  S1135-76062007000100006-1   \n",
       "1985  S1135-76062007000100006-1   \n",
       "1986  S1135-76062007000100006-1   \n",
       "1987  S1135-76062007000100006-1   \n",
       "1988  S1135-76062007000100006-1   \n",
       "\n",
       "                                                   text            label  \\\n",
       "0     Paciente de 69 a. de edad con un PSA en el mom...  DERIVED_FROM/TO   \n",
       "1     El paciente ten铆a una biopsia previa por sexta...  DERIVED_FROM/TO   \n",
       "2     Se practic贸 una E-RME que mostr贸 inicialmente ...  DERIVED_FROM/TO   \n",
       "3     Un paciente var贸n de 19 a帽os acudi贸 al Servici...  DERIVED_FROM/TO   \n",
       "4          No presentaba ning煤n antecedente urol贸gico.   DERIVED_FROM/TO   \n",
       "...                                                 ...              ...   \n",
       "1984  La madre hab铆a fallecido s煤bitamente a los 48 ...  DERIVED_FROM/TO   \n",
       "1985  Una noche sali贸 a cenar con los compa帽eros de ...  DERIVED_FROM/TO   \n",
       "1986  No manifest贸 ninguna sintomatolog铆a y se acost...  DERIVED_FROM/TO   \n",
       "1987  Poco despu茅s el perro comenz贸 a ladrar por lo ...  DERIVED_FROM/TO   \n",
       "1988  Se avis贸 a los servicios de emergencia 061 que...  DERIVED_FROM/TO   \n",
       "\n",
       "      start_offset  end_offset  \n",
       "0                0          79  \n",
       "1               79         140  \n",
       "2              140         479  \n",
       "3                0         112  \n",
       "4              112         156  \n",
       "...            ...         ...  \n",
       "1984           180         256  \n",
       "1985           256         369  \n",
       "1986           369         439  \n",
       "1987           439         537  \n",
       "1988           537         629  \n",
       "\n",
       "[1989 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"label\"] = original_labels\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e72c1ec6-43be-487d-892e-28cbc5b25147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entry_boundaries(df_test, note_id, boundaries):\n",
    "    temp_df = df_test[df_test[\"note_id\"]==note_id]\n",
    "    predictions = boundaries\n",
    "    for pred in predictions:\n",
    "        if pred[\"start_offset\"] in temp_df[\"start_offset\"].values:\n",
    "            pred[\"boundary\"] = temp_df[temp_df[\"start_offset\"]==pred[\"start_offset\"]][\"label\"].values[0]\n",
    "        else:\n",
    "            pred[\"boundary\"] = None\n",
    "    return predictions   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1052d34b-fc58-4693-8ef3-5baec76e3292",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"data/raw/clinais.dev.json\"\n",
    "\n",
    "with open(test_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "for note_id, entry in data[\"annotated_entries\"].items():\n",
    "    predictions[entry[\"note_id\"]] = entry\n",
    "    predictions[entry[\"note_id\"]][\"boundary_annotation\"][\"prediction\"] = entry_boundaries(df_test, note_id, entry[\"boundary_annotation\"][\"gold\"])\n",
    "    \n",
    "with open(\"data/predictions/predictions_rule_classsification.json\", \"w\") as f:\n",
    "    json.dump({\"annotated_entries\": predictions}, f)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
